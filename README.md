本项目使用了[Fashion-MNIST数据集]([Fashion MNIST (kaggle.com)](https://www.kaggle.com/datasets/zalando-research/fashionmnist))。数据集包含60,000个训练样本和10,000个测试样本。

### **以下是数据准备过程中的步骤：**

***\*（1）加载数据集：\**** 在本次任务中实现了读取数据集文件中的图像和标签的函数。图像加载为形状为(n, 784)的numpy数组，其中n是样本数量，784是28x28像素图像的展开维度。

***\*（2）独热编码：\**** 训练集和测试集的标签都转换成了独热编码向量。这种转换简化了交叉熵损失的计算，并且与模型的输出格式兼容。

***\*（3）划分验证集：\**** 为了在训练期间评估模型的性能，将训练数据的10%划分为验证集。

---

### **训练过程**

本项目中的模型使用交叉熵损失函数进行训练，并结合L2正则化以防止过拟合。训练过程包括：

***\*1、前向传播：\**** 输入数据经过各层，并在每层的输出后应用相应的激活函数：

（1）输入层的数据先与weights1相乘，并加上biases1后进入第一个隐藏层。

（2）第一个隐藏层的输出经过ReLU激活函数，然后与weights2相乘，加上biases2进入第二个隐藏层。

（3）第二个隐藏层的输出经过ReLU激活函数，然后与weights3相乘，加上biases3进入输出层。

（4）输出层的数据通过softmax激活函数，生成10个数字类别的概率分布。

***\*2、反向传播：\**** 训练中模型通过反向传播算法调整权重。具体步骤如下：

（1）输出误差计算：通过交叉熵损失函数计算实际标签和预测结果之间的差距。

（2）梯度计算：根据误差计算各层梯度：weights3的梯度由输出层误差与第二个隐藏层输出的转置矩阵相乘得到；weights2和weights1的梯度通过类似方式计算，包括各自偏置的梯度。

（3）L2正则化：在更新权重时，L2正则化通过惩罚权重值，防止过拟合。

（4）权重更新：使用学习率从0.001到0.1不等，对每层的权重和偏置进行更新。

***\*3、批次训练：\**** 为提升训练速度和泛化能力，模型以32个样本为一批进行训练。在每个epoch期间，整个训练集被随机打乱并分成批次，每个批次都进行前向传播、损失计算、反向传播和权重更新。

***\*4、性能评估：\**** 每个epoch结束时，在验证集上评估模型性能，包括验证损失和准确率。

---

### **评估结果**

为找到最佳配置，对多种超参数组合进行了测试：

***\*1、隐藏层大小：\**** 第一层和第二层的隐藏层分别测试了64、128、256单元的配置。

***\*2、学习率：\**** 测试了从0.001到0.1的不同学习率。

***\*3、L2正则化：\**** 测试了从0.001到0.1的正则化强度。

---

### **可视化模型权重**

为更好地了解模型的学习过程，对模型的权重进行了可视化。

***\*1. 权重热图\****

本次任务中绘制了权重热图，用以展示模型各层权重的分布情况。通过观察热图，可以发现权重矩阵中的各个元素之间的大小关系，以及权重分布的整体特征。

***\*2. 权重直方图\****

本次任务中绘制了权重直方图，以更直观地展示权重的分布情况。通过直方图，可以观察到权重值的分布情况，包括权重值的中心趋势、偏度和尾部情况等。有助于了解模型的参数在训练过程中的变化和分布情况。

更多更清晰的结果详见github上的[“代码与结果展示.html“]([Homework_1/代码与结果展示.html at main · yujincao/Homework_1 (github.com)](https://github.com/yujincao/Homework_1/blob/main/代码与结果展示.html))
